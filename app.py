import os
from dotenv import load_dotenv
import streamlit as st
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma # <-- DEÄžÄ°ÅžTÄ°
from langchain_community.document_loaders import TextLoader # <-- DEÄžÄ°ÅžTÄ°
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate # <-- DEÄžÄ°ÅžTÄ°
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Ortam deÄŸiÅŸkenlerini yÃ¼kle (.env dosyasÄ±ndan) ---
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    st.error("âŒ GOOGLE_API_KEY .env dosyasÄ±nda bulunamadÄ±. LÃ¼tfen API anahtarÄ±nÄ± ekle.")
    st.stop()

# --- Streamlit sayfa baÅŸlÄ±ÄŸÄ± ---
st.set_page_config(page_title="Discover TÃ¼rkiye Chatbot", page_icon="ðŸ‡¹ðŸ‡·")
st.title("ðŸ‡¹ðŸ‡· Discover TÃ¼rkiye Chatbot")
st.write("TÃ¼rkiyeâ€™deki ÅŸehirleri keÅŸfedin! Bana sorular sorun, sohbet edelim.")

# --- Veri YÃ¼kleme ve Ä°ÅŸleme  ---
# Bu fonksiyon, verilerin olduÄŸu dosyadan verileri alÄ±r
# Uzun metinleri 1000 karakterlik parÃ§alara bÃ¶ler 
# Her parÃ§ayÄ± â€œembeddingâ€ haline getirip Chroma veritabanÄ±na ekler
# Bu iÅŸlem cache'lenir, bÃ¶ylece uygulama her yeniden Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda tekrarlanmaz
@st.cache_resource
def load_and_process_data():
    loader = TextLoader("turkiye_turizm.txt", encoding='utf-8')
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = text_splitter.split_documents(documents)  
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)
    vectordb = Chroma.from_documents(docs, embeddings)
    return vectordb

vectordb = load_and_process_data() #Embeddingâ€™lenmiÅŸ verileri iÃ§erir

# --- Prompt ÅžablonlarÄ± ---
# Bu bÃ¶lÃ¼mde modelin nasÄ±l dÃ¼ÅŸÃ¼neceÄŸini ve cevap vereceÄŸini tanÄ±mlayan iki ayrÄ± "prompt ÅŸablonu" oluÅŸturuluyor
# AmaÃ§, modeli iki aÅŸamalÄ± dÃ¼ÅŸÃ¼ndÃ¼rmektir:
# CONDENSE_QUESTION_PROMPT â†’ KullanÄ±cÄ±nÄ±n takip sorularÄ±nÄ± geÃ§miÅŸ baÄŸlama gÃ¶re yeniden yazar (soruyu anlamlÄ± hale getirir)
# QA_PROMPT â†’ Modelin cevabÄ± hangi Ã¼slupta, hangi kaynaklara dayanarak vereceÄŸini belirler
# BÃ¶ylece model hem "neyi sorduÄŸunu" hem de "nasÄ±l cevaplayacaÄŸÄ±nÄ±" ayrÄ± ayrÄ± Ã¶ÄŸrenir

# 1. Soru YoÄŸunlaÅŸtÄ±rma Prompt'u
_template = """
AÅŸaÄŸÄ±daki sohbet geÃ§miÅŸi ve takip sorusu verildiÄŸinde, takip sorusunu, sohbet geÃ§miÅŸindeki ÅŸehir adÄ± gibi ana konuyu MUTLAKA iÃ§erecek ÅŸekilde, tek baÅŸÄ±na bir soru olarak yeniden ifade et.

Ã–rnek:
Sohbet GeÃ§miÅŸi:
KullanÄ±cÄ±: Konya'da ne yenir?
Yapay Zeka: Konya'da etli ekmek yiyebilirsiniz.
Takip Sorusu: peki orada nereler gezilir?
Tek BaÅŸÄ±na Soru: Konya'da nereler gezilir?

Sohbet GeÃ§miÅŸi:
{chat_history}
Takip Sorusu: {question}
Tek BaÅŸÄ±na Soru:"""
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

# 2. Cevaplama Prompt'u
QA_PROMPT_TEMPLATE = """
Sen TÃ¼rkiye hakkÄ±nda her ÅŸeyi bilen, arkadaÅŸ canlÄ±sÄ± ve yardÄ±msever bir turizm rehberisin.
Sana verilen metin parÃ§alarÄ±nÄ± ({context}) kullanarak son soruyu ({question}) cevapla.
CevaplarÄ±nÄ± SADECE verilen metindeki bilgilere dayanarak oluÅŸtur.
Bilgileri birleÅŸtirerek, tekrar etmeden, akÄ±cÄ± bir paragraf halinde sun.
CevabÄ±n sanki bir rehberle sohbet ediyormuÅŸ gibi sÄ±cak ve doÄŸal olsun. EÄŸer bilgi metinde yoksa veya soru konu dÄ±ÅŸÄ±ysa, "Bu konuda elimde bilgi yok, ama istersen TÃ¼rkiye'deki diÄŸer ÅŸehirlerle ilgili Ã¶nerilerde bulunabilirim." de.

YardÄ±msever CevabÄ±n:
"""
QA_PROMPT = PromptTemplate(
    template=QA_PROMPT_TEMPLATE, input_variables=["context", "question"]
)

# --- Dil Modeli (LLM), HafÄ±za ve Sohbet Zinciri ---
# Burada LangChain'e hangi yapay zekÃ¢ modelini (LLM) kullanacaÄŸÄ±nÄ± sÃ¶ylÃ¼yoruz
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", # KullanÄ±lan Gemini sÃ¼rÃ¼mÃ¼nÃ¼ belirler
    google_api_key=google_api_key, # API eriÅŸim anahtarÄ±nÄ± iletir
    temperature=0.7, # Modelin yaratÄ±cÄ±lÄ±ÄŸÄ±nÄ± ayarlar
    convert_system_message_to_human=True # LangChainâ€“Streamlit uyumunu saÄŸlar
)

# --- HafÄ±za, Bilgi Arama ve Sohbet Zinciri ---
# Bu bÃ¶lÃ¼m chatbot'un sohbet geÃ§miÅŸini hatÄ±rlamasÄ±nÄ±, veritabanÄ±nda arama yapmasÄ±nÄ± ve cevap Ã¼retmesini saÄŸlar

# HafÄ±za (Memory):
# - st.session_state.memory â†’ KonuÅŸma geÃ§miÅŸi oturum bazlÄ± olarak saklanÄ±r
# - ConversationBufferMemory â†’ Ã–nceki mesajlarÄ± sÄ±ralÄ± ÅŸekilde tutar (baÄŸlam korunur)
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Retriever:
# - vectordb.as_retriever(k=4) â†’ Chroma veritabanÄ±nda en alakalÄ± 4 belgeyi getirir
# - Bu belgeler embedding benzerliÄŸine gÃ¶re seÃ§ilir
retriever = vectordb.as_retriever(search_kwargs={'k': 4})

# ConversationalRetrievalChain:
# - LLM (Gemini) + Retriever + Memory birleÅŸimiyle Ã§alÄ±ÅŸÄ±r
# - condense_question_prompt â†’ Takip sorularÄ±nÄ± baÄŸlama gÃ¶re yeniden yazar
# - QA_PROMPT â†’ Belgelerden gelen bilgiyi rehber Ã¼slubuyla kullanÄ±cÄ±ya sunar
conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=st.session_state.memory,
    condense_question_prompt=CONDENSE_QUESTION_PROMPT,
    combine_docs_chain_kwargs={"prompt": QA_PROMPT}
)

# --- Streamlit Sohbet ArayÃ¼zÃ¼ ---
# Bu kÄ±sÄ±m kullanÄ±cÄ± ile chatbot arasÄ±ndaki gÃ¶rsel sohbet deneyimini oluÅŸturur.

# GeÃ§miÅŸ Mesajlar:
# - st.session_state.messages â†’ Oturum boyunca mesaj geÃ§miÅŸini saklar.
# - EÄŸer yoksa boÅŸ liste oluÅŸturulur.
# - Daha Ã¶nceki tÃ¼m konuÅŸmalar for dÃ¶ngÃ¼sÃ¼yle ekranda gÃ¶rÃ¼ntÃ¼lenir.
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ± Girdisi:
# - st.chat_input() â†’ KullanÄ±cÄ±nÄ±n mesaj yazdÄ±ÄŸÄ± alan.
# - KullanÄ±cÄ± mesaj gÃ¶nderdiÄŸinde hem ekrana yazÄ±lÄ±r hem geÃ§miÅŸe eklenir.
if prompt := st.chat_input("Ä°stanbul hakkÄ±nda bir soru sorun..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

# Chatbot CevabÄ±:
# - st.spinner() â†’ "DÃ¼ÅŸÃ¼nÃ¼yorum..." animasyonu gÃ¶sterir.
# - conversation_chain() â†’ Model, veritabanÄ± ve hafÄ±zayÄ± kullanarak cevabÄ± Ã¼retir.
# - Cevap ekranda gÃ¶sterilir ve geÃ§miÅŸe kaydedilir.
    with st.chat_message("assistant"):
        with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
            try:
                result = conversation_chain({"question": prompt})
                response = result["answer"]
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"âŒ Bir hata oluÅŸtu: {str(e)}")




